<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>WMD: From Word Embeddings To Document Distances</title>
    <url>/2020/06/04/WMD-From-Word-Embeddings-To-Document-Distances/</url>
    <content><![CDATA[<p>本文是对论文<a href="http://proceedings.mlr.press/v37/kusnerb15.pdf" target="_blank" rel="noopener">From Word Embeddings To Document Distances</a>的总结。该论文基于word2vec embedding 和EMD（Earth Mover’s Distance）提出了一种新的计算文档距离的算法WMD（Word Mover’s Distance）。旨在解决 <em>Obama speaks to the media in Illinois</em> 和 <em>The President greets the press in Chicago</em> 仅仅因词语拼写不同而导致距离很远的不合理现象。尽管这两个句子对应的词语在语义上是相近的。</p>
<a id="more"></a>

<p>本文的主要内容包括：</p>
<ul>
<li>WMD模型</li>
<li>WMD模型的速度优化</li>
<li>总结</li>
</ul>
<p>在WMD模型提出以前，计算文档相似度主要是使用词袋模型或TF-IDF。这两种方法都存在严重的数据稀疏问题，而且完成没有考虑词与词之间的语义关系。对意义相近但用词不同的文档无能为力。</p>
<h3 id="Word-Mover’s-Distance（WMD）模型"><a href="#Word-Mover’s-Distance（WMD）模型" class="headerlink" title="Word Mover’s Distance（WMD）模型"></a>Word Mover’s Distance（WMD）模型</h3><hr>
<p>MWD既考虑了词袋模型的词频也考虑了词与词之间的距离：</p>
<h4 id="文档表示（nBOW）"><a href="#文档表示（nBOW）" class="headerlink" title="文档表示（nBOW）"></a>文档表示（nBOW）</h4><p>WMD中文档表示就是归一化的词袋模型，文档向量各分量值形式化表示为：<br>$$<br>d _ { i } = \frac { c _ { i } } { \sum _ { i = 1 } ^ { n } c _ { j } }<br>$$<br>分子为词频，分母为文档总词数。以论文中的例句为例，去停后可表示为：</p>
<ul>
<li>D0：President greets press Chicago [0.25, 0.25, 0.25, 0.25, 0, 0, …]</li>
<li>D1：Obama speaks media Illinois [0, …, 0.25, 0.25, 0.25, 0.25, 0, …]</li>
<li>D3：Obama speaks Illinois [0, .., 0.33, 0.33, 0.33, 0, …, 0]</li>
</ul>
<h4 id="词转化损失（word-travel-cost）"><a href="#词转化损失（word-travel-cost）" class="headerlink" title="词转化损失（word travel cost）"></a>词转化损失（word travel cost）</h4><p>利用word2vec词向量计算欧式距离得到：<br>$$<br>c ( i , j ) = | x _ { i } - x _ { j } | _ { 2 }<br>$$<br>$c ( i , j ) $表示一个词转化为另一个词的损失。</p>
<h4 id="文档距离"><a href="#文档距离" class="headerlink" title="文档距离"></a>文档距离</h4><p>WMD文档距离是由文档表示（nBow）和词转化损失（word travel cost）共同作用得到的，计算方法为寻找两个文档中对应的词，累加各组对应词的（词转化损失 * nBow对应的分量）。对应词的标准是$c(i, j)$最小的词，如Figure 1所示。</p>
<p><img src="http://m.qpic.cn/psc?/V13FFsWG1SefP8/p4b63XAxeBU0dFwQicUt3CGkx6DnFyk.q1oiK0VT4CmULupL1*P.hj8AixoE7E5DXkt*fqv09R0MTU0jgh9CAA!!/b&bo=igL1AYoC9QEDByI!&rf=viewer_4" alt=""></p>
<center>Figure 1. (Top:) The components of the WMD metric between a
query D0 and two sentences D1, D2 (with equal BOW distance).
The arrows represent flow between two words and are labeled
with their distance contribution. (Bottom:) The flow between two
sentences D3 and D0 with different numbers of words. This mismatch causes the WMD to move words to multiple similar words.</center>

<p>当比较的两个文档长度相同时，文档距离可以简单理解为：<br>$$<br>W!M!D \  document \ distance = \sum _ {i=1}^ {n} d_{i} * min(c(i, j))<br>$$<br>当比较的两个文档长度不同时，我们需要引入矩阵$T \in R ^ { n \times n }$来表示文档距离：<br>$$<br>\sum _ { i , j = 1 } ^ { n } T _ { i j } c ( i , j )<br>$$<br>其中 $T _ { i j } \geq 0$，表示文档中的词$i$在多大程度上转化为另一个文档中的词 $j$。</p>
<p>举个例子，Figure 1中的$D_{0}$和$D_{3}$，当$T$ 矩阵为如下情况时，$D_{0}$和$D_{3}$会产生如图中所示的对应关系：<br>$$<br>\begin{array}{c|cccc}<br>         &amp; \text{President} &amp; \text{greets} &amp; \text{press} &amp; \text{Chicago}\<br>\hline<br>Obama    &amp; 0.25             &amp; 0.08           &amp; 0.          &amp; 0.             \<br>speaks   &amp; 0.08             &amp; 0.             &amp; 0.25        &amp; 0.            \<br>Illinois &amp; 0.08             &amp; 0.             &amp; 0.          &amp; 0.25          \<br>\end{array}<br>$$</p>
<center>
  Figure 2
</center>



<p>还记得$D_3$的nBow表示，<em>Obama<em>对应的分量值为0.33，这里</em>Obama<em>被距离最近的词</em>President</em> 分走了0.25, 还剩0.08需要分配出去。图中的情况，<em>greets</em>是距离<em>Obama</em>第二近的词，于是0.08被分配给了<em>greets</em>。以此类推。</p>
<p>显然，两个文档长度相同时，$T$ 除一一对应的词的位置是nbow分量值外，其他位置都是0.</p>
<p><em>注：Figure 1中$D_0$和$D_3$只是众多可能的对应关系的一种，并不是最优的对应关系。最优的对应关系如前面所说，寻找$c(i, j)$尽可能小的词对。一对多的情况下，按$c(i, j)$由小到大顺位取。</em></p>
<h4 id="WMD模型"><a href="#WMD模型" class="headerlink" title="WMD模型"></a>WMD模型</h4><p>至此，我们可以顺理成章的给出WMD模型的$Obj$：<br>$$<br>\begin{split}<br>\min _ { T \geq 0 } &amp;\sum _ { i , j = 1 } ^ { n } T _ { i j } c ( i , j ) \<br>subject\ to: &amp;\sum _ { j = 1 } ^ { n } T _ { i j } = d _ { i } \quad \forall i \in { 1 , \ldots , n } \qquad (1)\<br>&amp;\sum _ { i = 1 } ^ { n } T _ { i j } = d _ { j } ^ { \prime } \quad \forall j \in { 1 , \ldots , n } \qquad (2)<br>\end{split} \<br>$$<br>条件（1）就是$T$的行元素相加和等于$d_i$ <em>。如 Obama (0.33)</em> = <em>President (0.25)</em> + <em>greets (0.08)</em> + <em>press (0.)</em> + <em>Chicago (0.)</em>。</p>
<p>条件（2）就是$T$的列元素相加和等于$d_j$。如<em>Press (0.25)</em> = <em>Obama (0.)</em> + <em>speak (0.25)</em> + <em>Illinois (0.)</em>。</p>
<p><strong>Figure2中例子$president$是不满足条件（2）的，对这里的理解有一定的疑问。希望想明白的小伙伴给留言告知。</strong></p>
<p>从$\min _ { T \geq 0 } \sum _ { i , j = 1 } ^ { n } T _ { i j } c ( i , j )$可以看出，这是一个有条件的线性规则问题。所有线性规则问题都有最优解。</p>
<h3 id="WMD模型的速度优化"><a href="#WMD模型的速度优化" class="headerlink" title="WMD模型的速度优化"></a>WMD模型的速度优化</h3><hr>
<h4 id="Word-centroid-distance（WCD）"><a href="#Word-centroid-distance（WCD）" class="headerlink" title="Word centroid distance（WCD）"></a>Word centroid distance（WCD）</h4><p>计算WMD的复杂度太高，为$O ( p ^ { 3 } \log p )$。WCD的结果是WMD的下限，它是一些简单的矩阵运算，复杂度为$O (dp)$。其推导过程如下：<br>$$<br>\begin{split}<br>&amp;\sum _ { i , j = 1 } ^ { n } T _ { i j } c ( i , j ) = \sum _ { i , j = 1 } ^ { n } T _ { i j } | x _ { i } - x _ { j } ^ { \prime } | _ { 2 } \<br>= &amp;\sum ^ { n } | T _ { i j } ( x _ { i } - x _ { j } ^ { \prime } ) | _ { 2 } \geq | \sum ^ { n } T _ { i j } ( x _ { i } - x _ { j } ^ { \prime } ) | _{2} \<br>= &amp;| \sum _ { i = 1 } ^ { n } ( \sum _ { j = 1 } ^ { n } T _ { i j } ) x _ { i } - \sum _ { j = 1 } ^ { n } ( \sum _ { i = 1 } ^ { n } T _ { i j } ) x _ { j } ^ { \prime } | _ { 2 } \<br>= &amp;| \sum _ { i = 1 } ^ { n } d _ { i } x _ { i } - \sum _ { j = 1 } ^ { n } d _ { j } ^ { \prime } x _ { j } ^ { \prime } | _ { 2 } = | X d - X d ^ { \prime } | _ { 2 } \<br>\end{split}<br>$$</p>
<p>WCD的推导相对容易，在不等式的步骤是利用$\sqrt a + \sqrt b \geq \sqrt{a+b}$ 得到的。</p>
<p>得到两个文档的WMD下限$| X d - X d ^ { \prime } | _ { 2 }$，可知这两个文档的距离一定大于这个值。在很多情况下，可以直接排除掉这些下限都已经很大的文档，免去进一步的运算，减少计算量。</p>
<h4 id="Relaxed-word-moving-distance-（RWMD）"><a href="#Relaxed-word-moving-distance-（RWMD）" class="headerlink" title="Relaxed word moving distance （RWMD）"></a>Relaxed word moving distance （RWMD）</h4><p>虽然WCD速度很快，但计算出来的下限相比真实值还是差距比较大的。RWMD通过分别去掉WMD中的一个条件来得到两个下限$\ell _ { 1 } ( d , d ^ { \prime } )$和$\ell _ { 2 } ( d , d ^ { \prime } )$, 然后取$\ell _ { r } ( d , d ^ { \prime } ) = \max ( \ell _ { 1 } ( d , d ^ { \prime } ) , \ell _ { 2 } ( d , d ^ { \prime } ) )$。当去掉的是条件（2）时，RMWD形式化表示如下：<br>$$<br>\begin{split}<br>\min _ { T \geq 0 } &amp;\sum _ { i , j = 1 } ^ { n } T _ { i j } c ( i , j ) \<br>subject\ to: &amp;\sum _ { j = 1 } ^ { n } T _ { i j } = d _ { i } \quad \forall i \in { 1 , \ldots , n } \<br>\end{split} \<br>$$<br>RWMD的复杂度为$O(p^2)$。<strong>（Figure 2的情况满足RWMD）</strong></p>
<h4 id="Prefetch-and-prune"><a href="#Prefetch-and-prune" class="headerlink" title="Prefetch and prune"></a>Prefetch and prune</h4><ol>
<li>计算所有文档与目标文档的WCD并对结果进行排序，选择前k个文档作为k-nearest-neighbour。</li>
<li>计算这k个文档与目标文档的具体WMD。</li>
<li>依次计算其他文档与目标文档的RWMD，并将其与第k个文档WMD进行对比，超过的可以直接prune掉。</li>
<li>如果没超过，计算该文档与目标文档的WMD并对k-nearest-neighbour进行更新。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr>
<ul>
<li>WMD完美地将word2vec与传统的Bow结合起来，这在深度学习与传统机器学习算法结合方面做得都不是很好的今天，无疑是个很novel的idea.</li>
<li>在解决WMD复杂度过高的问题上，利用复杂度低很多的两个下限配合使用，既大大提高了速度，又没有损失精度。</li>
<li>对WMD模型的条件（2）目前还存在疑问，希望能尽快找到答案。</li>
</ul>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><hr>
<p><a href="http://proceedings.mlr.press/v37/kusnerb15.pdf" target="_blank" rel="noopener">From Word Embeddings To Document Distances</a></p>
<h3 id="文本距离类的其他资料"><a href="#文本距离类的其他资料" class="headerlink" title="文本距离类的其他资料"></a>文本距离类的其他资料</h3><hr>
<p><a href="http://www.deepsmart.ai/76.html" target="_blank" rel="noopener">CIKM Cup 2018阿里小蜜短文本匹配算法竞赛 – 冠军DeepSmart团队分享</a></p>
<p><a href="https://arxiv.org/pdf/1908.10084.pdf" target="_blank" rel="noopener">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a></p>
]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>document distance</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/05/23/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<a id="more"></a>

<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>XGBoost: A Scalable Tree Boosting System</title>
    <url>/2020/06/01/XGBoost-A-Scalable-Tree-Boosting-System/</url>
    <content><![CDATA[<p>本文是对XGBoost的经典论文<a href="https://dl.acm.org/doi/pdf/10.1145/2939672.2939785" target="_blank" rel="noopener">XGBoost: A Scalable Tree Boosting System</a>的总结。<a href="https://github.com/dmlc/xgboost" target="_blank" rel="noopener">XGBoost</a> 的全称是 eXtreme Gradient Boosting。作者为华盛顿大学研究机器学习的大牛<a href="http://homes.cs.washington.edu/~tqchen/" target="_blank" rel="noopener">陈天奇</a>，其最大的特点在于，它能够自动利用 CPU 的多线程进行并行，同时在算法上加以改进提高了精度。</p>
<p>本文更多关注算法，主要内容如下：</p>
<a id="more"></a>

<ul>
<li>XGBoost目标函数推导</li>
<li>XGBoost防止过拟合策略</li>
<li>切分搜索算法</li>
<li>XGBoost针对稀疏问题的处理</li>
<li>系统速度优化</li>
</ul>
<h3 id="XGboost目标函数推导"><a href="#XGboost目标函数推导" class="headerlink" title="XGboost目标函数推导"></a>XGboost目标函数推导</h3><hr>
<h4 id="预测值"><a href="#预测值" class="headerlink" title="预测值"></a>预测值</h4><p>XGboost是基于回归树的集成学习模型。集成学习主要有两种方式：bagging和boosting。XGboost属于boosting, 其预测值通过对各弱分类器的结果求和得到。对于输入$x_{i}$：<br>$$<br>\begin{split}\hat { y } _ { i } = \phi ( x _ { i } ) = \sum _ { k = 1 } ^ { K } f _ { k } ( x _ { i } ) , \quad f _ { k } \in F  \qquad (1)<br>\end{split}<br>$$<br>其中，$f _ {k}$即为一个弱分类器，在XGboost中，弱分类器为 CART。<em>F</em> 为 CART 空间。  </p>
<h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>为了学习得到上式中的K个$f_{k}$,  我们需要通过最小化模型的目标函数来获得。目标函数通常由损失+正则项表示。损失指示离真实值的差距，正则防止过拟合。这里损失为$\begin{split}\sum _{ i } l ( \hat y _ { i }, y)\end{split}$, 是个可微凸函数。正则项则是K个$f _ {k}$中的参数。由于XGBoost的$f _ {k}$是回归树，该目标函数如下：</p>
<p>$$<br>\begin{split}&amp;O b j = \sum _ { i } l ( \hat { y } _ { i } , y _ { i } ) + \sum _ { k } \Omega ( f _ { k } ) \<br>&amp;where\quad\Omega ( f ) = \gamma T + \frac { 1 } { 2 } \lambda | w | ^ { 2 } \nonumber \end{split} \qquad (2)<br>$$<br>其中，$\Omega( f ) $ 是惩罚项。$f_{ k }$做为回归树，影响其复杂度的参数为叶子节点的个数$, T ,$及叶子节点的权重$w$.</p>
<h4 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h4><p>由于$f _ { k }$是树，与其他模型$f$ 可以由数值型向量表示不同，我们不能使用往常的优化算法，如SGD，来寻找$f$。这里的解决方案是Additive Training（Boosting）。</p>
<p>$$<br>\begin{split} &amp;\hat { y } _ { i } ^ { ( 0 ) } = 0  \<br>&amp;\hat { y } _ { i } ^ { ( 1 ) } = f _ { 1 } ( x _ { i } ) = \hat { y } _ { i } ^ { ( 0 ) } + f _ { 1 } ( x _ { i } )  \<br>&amp;\hat { y } _ { i } ^ { ( 2 ) } = f _ { 1 } ( x _ { i } ) + f _ { 2 } ( x _ { i } ) = \hat { y } _ { i } ^ { ( 1 ) } + f _ { 2 } ( x _ { i } )  \<br>&amp; …  \<br>&amp;\hat { y } _ { i } ^ { ( t ) } = \sum _ { k = 1 } ^ { t } f _ { k } ( x _ { i } ) = \hat { y } _ { i } ^ { ( t - 1 ) } + f _ { t } ( x _ { i } )<br>\end{split}<br>$$<br>其中，每一轮添加的$f$ 是最优化目标函数的结果。</p>
<h4 id="目标函数近似形式"><a href="#目标函数近似形式" class="headerlink" title="目标函数近似形式"></a>目标函数近似形式</h4><p>根据Additive Training方式，在第$t$ 轮，$\hat { y } _ { i } ^ { ( t ) } = \hat { y } _ { i } ^ { ( t - 1 ) } + f _ { t } ( x _ { i } )$。 由此在第$t$轮的目标函数可以改写为：</p>
<p>$$<br>\begin{split}<br>O b j ^ { ( t ) } &amp;= \sum _ { i = 1 } ^ { n } l ( y _ { i } , \hat { y } _ { i } ^ { ( t ) } ) + \sum _ { i = 1 } ^ { t } \Omega ( f _ { i } )  \<br>&amp;= \sum _ { i = 1 } ^ { n } l ( y _ { i } , \hat { y } _ { i } ^ { ( t - 1 ) } + f _ { t } ( x _ { i } ) ) + \Omega ( f _ { t } ) + constant<br>\end{split}<br>$$<br>这个目标函数仍然很难求最优。我们用泰勒展开取其近似如下：</p>
<p>$$<br>O b j ^ { ( t ) } \simeq \sum _ { i = 1 } ^ { n } [ l ( y _ { i } , \hat { y } _ { i } ^ { ( t - 1 ) } ) + g _ { i } f _ { t } ( x _ { i } ) + \frac { 1 } { 2 } h _ { i } f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) + constant<br>$$<br>其中，$g _ { i } = \partial _ { \hat { y } ^ { ( t - 1 ) } } l ( y _ { i } , \hat { y } ^ { ( t - 1 ) } ) , \quad h _ { i } = \partial _ { \hat { y } ^ { ( t - 1 ) } } ^ { 2 } l ( y _ { i } , \hat { y } ^ { ( t - 1 ) } )$,  $g _ {i}$, $h _ {i}$分别为 $l ( y _ { i } , \hat { y } _ { i } ^ { ( t - 1 ) } )$对$\hat y ^ {(t - 1)}$求偏导的一阶和二阶导数。把上式中的常数项去掉，得：</p>
<p>$$<br>\tilde {  O b j ^ { ( t ) } } = \sum _ { i = 1 } ^ { n } [ g _ { i } f _ { t } ( x _ { i } ) + \frac { 1 } { Z } h _ { i } f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) \qquad (3)<br>$$<br>这里要注意理解上式是针对每一个样本$x_{i}$求损失，然后求所有样本损失和。我们将要在接下来的计算中为了方便计算$\tilde {  O b j ^ { ( t ) } }$的最优值而换一种对这波操作的形式化表达方式。</p>
<h4 id="目标函数求最优"><a href="#目标函数求最优" class="headerlink" title="目标函数求最优"></a>目标函数求最优</h4><p>这里定义 $I _ { j } = { i | q ( x _ { i } ) = j }$。$I_{j}$为被树分配到叶子节点$j$ 上的样本集。$q ( x _ { i } )$为样本$x _ {i}$通过q这个树结构到达叶子节点$j$。 $w_ {j}$为第$j$个叶子节点的权重。则 $f _ { t } ( x _ { i } ) = w _ { q ( x _ { i } ) }$。因此式 $(3)$ 可以表示为：<br>$$<br>\begin{split}<br>O b j ^ { ( t ) } &amp;\simeq \sum _ { i = 1 } ^ { n } [ g _ { i } f _ { t } ( x _ { i } ) + \frac { 1 } { 2 } h _ { i } f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) \<br>&amp;= \sum _ { i = 1 } ^ { n } [ g _ { i } w _ { q ( x _ { i } ) } + \frac { 1 } { 2 } h _ { i } w _ { q ( x _ { i } ) } ^ { 2 } ] + \gamma T + \lambda \frac { 1 } { 2 } \sum _ { j = 1 } ^ { T } w _ { j } ^ { 2 } \<br>&amp;= \sum _ { j = 1 } ^ { T } [ ( \sum _ { i \in I _ { j } } g _ { i } ) w _ { j } + \frac { 1 } { 2 } ( \sum _ { i \in I _ { j } } h _ { i } + \lambda ) w _ { j } ^ { 2 } ] + \gamma T<br>\end{split}<br>$$<br>中括号里为我们求最优解时最喜欢形式$ax ^ 2 + bx$。显然，当</p>
<p>$$<br>w _ { j } ^ { * } = - \frac { \sum _ { i \in I _ { j } } g _ { i } } { \sum _ { i \in I _ { j } } h _ { i } + \lambda }<br>$$<br>时，得到最优的$\tilde {O b j ^ { ( t ) }}$:</p>
<p>$$<br>\tilde { Obj } ^ { ( t ) }  = - \frac { 1 } { 2 } \sum _ { j = 1 } ^ { T } \frac { ( \sum _ { i \in I _ { j } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { j } } h _ { i } + \lambda } + \gamma T \qquad (4)<br>$$<br>至此，我们完成了求第$t$轮时目标函数的最优值。这个值可以用来衡量树结构的好坏。也就是说，判断一棵树的质量，我们只要算出$g _ {i}$, $h_{i}$，然后代入式子$(4)$即可算出。</p>
<h3 id="防止过拟合策略"><a href="#防止过拟合策略" class="headerlink" title="防止过拟合策略"></a>防止过拟合策略</h3><hr>
<ul>
<li>权重衰减（Shrinkage）: 在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型。</li>
<li>列采样（Column subsampling）: 同对数据集做有放回的采样可以防止拟合一样，对特征做有放回的采样也可以防止过拟合。有两种方式：建树前采样和节点分裂之前采样。不仅如此，此方法还可以加快模型计算速度。</li>
</ul>
<h3 id="切分搜索算法"><a href="#切分搜索算法" class="headerlink" title="切分搜索算法"></a>切分搜索算法</h3><hr>
<p>当我们把一个节点做了切分后，我们需要一个评估这个切分是好是坏的标准。一个朴素的思想就是切分前节点的目标函数的得分<strong>减去</strong>切分后左右两节点目标函数之和的得分，如果两者的差距较大，说明切分后的损失较小。计算公式如下：<br>$$<br>\begin{split}<br>L _ { s p l i t } &amp;= L ^ { ( t ) } - ( L _ { l } ^ { ( t ) } + L _ { r } ^ { ( t ) } ) \<br>&amp;= - \frac { 1 } { 2 } \frac { ( \sum _ { i \in I _ { j } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { j } } h _ { i } + \lambda } + \gamma - ( - \frac { 1 } { 2 } \frac { ( \sum _ { i \in I _ { I } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { l } } h _ { i } + \lambda } + - \frac { 1 } { 2 } \frac { ( \sum _ { i \in I _ { r } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { r } } h _ { i } + \lambda } + 2 \gamma )\<br>&amp;= \frac { 1 } { 2 } [ \frac { ( \sum _ { i \in I _ { L } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { L } } h _ { i } + \lambda } + \frac { ( \sum _ { i \in I _ { R } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { R } } h _ { i } + \lambda } - \frac { ( \sum _ { i \in I } g _ { i } ) ^ { 2 } } { \sum _ { i \in I } h _ { i } + \lambda } ] - \gamma<br>\end{split} \qquad (5)<br>$$</p>
<h4 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h4><p>遍历所有特征的所有可能切分点，计算（5）式，取得分最高的切分点进行切分。</p>
<h4 id="近似算法"><a href="#近似算法" class="headerlink" title="近似算法"></a>近似算法</h4><p>近似算法按特征分布的百分位提出候选切分点，相当于将特征的特征值分桶后再遍历。按提出候选切分点的时间不同，有两种切分的方式：全局切分和局部切分。</p>
<p>全局切分就是在建树之初就提出了每个特征的候选切分点，之后对树的所有节点做切分时都从这些节点对应的特征候选切分点中选择。</p>
<p>局部切分是在每次节点切分后优化提出候选切分点。候选切分点提出较全局切分频繁，较全局切分需要较少切分点。适合生成较深的树。</p>
<p>全局切分候选切分点足够多时，可以达到和局部切分差不多的效果。</p>
<h4 id="加权分位点"><a href="#加权分位点" class="headerlink" title="加权分位点"></a>加权分位点</h4><p>加权分位点是近似算法的一种。上面提到的近似算法通常是将切分点设在使数据均匀分布的位置。而加权分位点则是将切分点设在使分桶后的数据对loss产生均衡影响的位置。如果分4个桶，加权分位点使得每个桶内的样本集对loss的影响持平为25%。</p>
<p>样本对loss的影响计算方法如下：<br>$$<br>\begin{split}<br>L ^ { ( t ) } &amp;\simeq \sum _ { i = 1 }^ { n } [ l ( y _ { i } , \hat { y } _ { i } ^ { ( t - 1 ) } ) + g _ { i } f _ { t } ( x _ { i } ) + \frac { 1 } { 2 } h _ { i } f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) + constant \<br>&amp;= \sum _ { i = 1 } ^ { n } [ \frac { 1 } { 2 } h _ { i } \cdot \frac { 2 \cdot g _ { i } f _ { t } ( x _ { i } ) } { h _ { i } } + \frac { 1 } { 2 } h _ { i } \cdot f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) + constant \<br>&amp;= \sum _ { i = 1 } ^ { n } \frac { 1 } { 2 } h _ { i } [ 2 \cdot \frac { g _ { i } } { h _ { i } } \cdot f _ { t } ( x _ { i } ) + f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) + constant \<br>&amp;= \sum _ { i = 1 } ^ { n } \frac { 1 } { 2 } h _ { i } [ ( 2 \cdot \frac { g _ { i } } { h _ { i } } \cdot f _ { t } ( x _ { i } ) + f _ { t } ^ { 2 } ( x _ { i } ) + ( \frac { g _ { i } } { h _ { i } } ) ^ { 2 } ) - ( \frac { g _ { i } } { h _ { i } } ) ^ { 2 } ] + \Omega ( f _ { t } ) + constant \<br>&amp;= \sum _ { i = 1 } ^ { n } \frac { 1 } { 2 } h _ { i } [ ( f _ { t } ( x _ { i } ) + \frac { g _ { i } } { h _ { i } } ) ^ { 2 } ] + \Omega ( f _ { t } ) + constant \<br>&amp;= \sum _ { i = 1 } ^ { n } \frac { 1 } { 2 } h _ { i } ( f _ { t } ( x _ { i } ) - ( - g _ { i } / h _ { i } ) ) ^ { 2 } + \Omega ( f _ { t } )<br>\end{split}<br>$$<br>这里和原文给出的公式差个负号，欢迎想明白的小伙伴留言。</p>
<p>从上式中我们可以看到样本$x _ {i}$的预测值$f _ { t } ( x _ { i })$对 $y = - g _ { i } / h _ { i }$的加权平方损失。权重即为$h _ {i}$，表示样本对loss的影响。</p>
<p>节点上特征值小于 $z$ 的样本集相对于节点上总样本集对loss影响的比重为：<br>$$<br>r _ { k } ( z ) = \frac { 1 } { \sum _ { ( x , h ) \in D _ { k } } h } \sum _ { ( x , h ) \in D _ { k } , x &lt; z } h<br>$$<br>我们的目标是要找到这样一组切分点${ s _ { k 1 } , s _ { k 2 } , \cdots s _ { k l } }$, 使用任意两个切分点之间的样本集对loss影响的比重小于一个数值$\epsilon$。比如 $\epsilon = 10%$，意思就是将特征值分为10桶，每桶内对应的样本集对loss的影响小于10%。形式化表示如下：<br>$$<br>| r _ { k } ( s _ { k , j } ) - r _ { k } ( s _ { k , j + 1 } ) | &lt; \epsilon , \quad s _ { k 1 } = \min _ { i } x _ { i k } , s _ { k l } = \max _ { i } x _ { i k }<br>$$</p>
<h3 id="稀疏问题的处理方法"><a href="#稀疏问题的处理方法" class="headerlink" title="稀疏问题的处理方法"></a>稀疏问题的处理方法</h3><hr>
<p>在实践中经常遇到的数据稀疏，某些样本的某些特征值是缺失的（NULL）或大量特征的特征值为0。对这种情况论文提出统一将这种数据划分到一个默认的分支，或左树或右树，然后分别计算损失，选择较优的那一个。</p>
<h3 id="XGBoost系统优化"><a href="#XGBoost系统优化" class="headerlink" title="XGBoost系统优化"></a>XGBoost系统优化</h3><hr>
<ul>
<li><p>针对并行计算的列块</p>
<ul>
<li><p>分<code>block</code>，便于并行化</p>
</li>
<li><p>分列存放，便于按列切片，实施 “列采样”</p>
</li>
<li><p>按照特征值事先排好序，便于执行切分</p>
</li>
<li><ul>
<li>在乱序列表上查一个数，只能是线性扫描，复杂度 $O ( N )$</li>
<li>在有序列表上可以做二分查找， $O ( \log N )$</li>
</ul>
</li>
</ul>
</li>
<li><p>针对 cpu缓存优化</p>
<ul>
<li>贪心算法：缓存预取缓解缓存未命中问题</li>
<li>近似算法：减小块的大小来解决缓存未命中问题</li>
</ul>
</li>
<li><p>针对IO优化</p>
<ul>
<li>Block Compression：block按列压缩，在加载到内存的过程中完成解压缩。</li>
<li>Block Sharding：将数据分片到多个磁盘上。 将预取线程分配给每个磁盘，并将数据取入内存缓冲区中。 然后，训练线程可以从每个缓冲区读取数据。 当有多个磁盘可用时，这有助于提高磁盘读取的吞吐量。</li>
</ul>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><hr>
<p>个人觉得XGBoost在面对目标函数很难被计算时的思路很有些意思。在看论文的过程中几次被这种奇思妙想击中，总结一下以便时不时回顾感叹一下：</p>
<ul>
<li>惩罚项是棵树时的处理</li>
<li>泰勒展示恰到好处的应用</li>
<li>$I_{j}$ 的定义把$Obj$ 神奇地转化成了二次表达式</li>
</ul>
<p>此外，在给特征值分桶时，用对loss的影响做为标准也是个不错的想法，并且找到用$h$做为权重也是漂亮的一波操作。</p>
<p>以上是个人觉得这篇论文最为出彩的地方，欢迎小伙伴留言分享你的心得。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><hr>
<ol>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/2939672.2939785" target="_blank" rel="noopener">XGBoost: A Scalable Tree Boosting System</a></li>
<li><a href="https://netman.aiops.org/~peidan/ANM2018/3.MachineLearningBasics/LectureCoverage/18.xgboost.pdf" target="_blank" rel="noopener">PPT</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/89546007" target="_blank" rel="noopener">详解《XGBoost: A Scalable Tree Boosting System》</a></li>
</ol>
]]></content>
      <tags>
        <tag>XGBoost</tag>
        <tag>Machine leanring</tag>
      </tags>
  </entry>
</search>
