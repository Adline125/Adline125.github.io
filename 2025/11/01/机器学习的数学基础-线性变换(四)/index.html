<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="在前面的章节中，我们已经看到，线性变换可以被认为是对由基向量确定的网格进行扭曲。 根据我们的几何直觉，我们推测测量变换对体积和距离的扭曲程度可以提供一些有价值的见解。正如我们将在本章中看到的，情况确实如此。保持距离或范数不变的变换很特殊，由此衍生出诸如主成分分析之类的方法。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习的数学基础-线性变换(四)">
<meta property="og:url" content="http://yoursite.com/2025/11/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2(%E5%9B%9B)/index.html">
<meta property="og:site_name" content="Adline125&#39;s Blog">
<meta property="og:description" content="在前面的章节中，我们已经看到，线性变换可以被认为是对由基向量确定的网格进行扭曲。 根据我们的几何直觉，我们推测测量变换对体积和距离的扭曲程度可以提供一些有价值的见解。正如我们将在本章中看到的，情况确实如此。保持距离或范数不变的变换很特殊，由此衍生出诸如主成分分析之类的方法。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p140figure4.14.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p141figure4.15.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p141formula1.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p141figure4.16.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p142formula3.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p142formula4.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p143formula1(4.9).png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p143figure4.17.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p143formula2(4.10).png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p145formula2.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p145figure4.18.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p146formula1.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p146formula2.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p146formula3(4.11).png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p146formula4.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p146figure4.19.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p147formula1.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p147formula2(4.12).png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p147formula4.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p148formula1(4.13).png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p148formula2.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p150formula1.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p150formula4.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p151formula1.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p151formula2.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p152formula1.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p152formula2.png">
<meta property="og:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p153formula1.png">
<meta property="article:published_time" content="2025-11-01T02:04:11.000Z">
<meta property="article:modified_time" content="2025-11-26T05:22:26.922Z">
<meta property="article:author" content="Adline125">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/Users/adline/%E7%BF%BB%E8%AF%91/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/pics2/p140figure4.14.png">

<link rel="canonical" href="http://yoursite.com/2025/11/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2(%E5%9B%9B)/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>机器学习的数学基础-线性变换(四) | Adline125's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Adline125's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Adline125's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">NLP Engineer, Google Developers Expert</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2025/11/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2(%E5%9B%9B)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/icon.png">
      <meta itemprop="name" content="Adline125">
      <meta itemprop="description" content="北京程序媛一枚，主攻NLP.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Adline125's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习的数学基础-线性变换(四)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-11-01 10:04:11" itemprop="dateCreated datePublished" datetime="2025-11-01T10:04:11+08:00">2025-11-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-11-26 13:22:26" itemprop="dateModified" datetime="2025-11-26T13:22:26+08:00">2025-11-26</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>在前面的章节中，我们已经看到，线性变换可以被认为是对由基向量确定的网格进行扭曲。</p>
<p>根据我们的几何直觉，我们推测测量变换对体积和距离的扭曲程度可以提供一些有价值的见解。正如我们将在本章中看到的，情况确实如此。保持距离或范数不变的变换很特殊，由此衍生出诸如主成分分析之类的方法。</p>
<a id="more"></a>
<p>本文的主要内容包括：</p>
<ul>
<li><strong>线性变换如何缩放区域</strong></li>
<li><strong>行列式的多重线性</strong></li>
<li><strong>行列式的基本性质</strong></li>
</ul>
<h2 id="行列式线性变换如何影响体积">1 行列式，线性变换如何影响体积</h2>
<p>在前面的章节中，我们已经看到，线性变换可以被认为是对由基向量确定的网格进行扭曲。</p>
<p>根据我们的几何直觉，我们推测测量变换对体积和距离的扭曲程度可以提供一些有价值的见解。正如我们将在本章中看到的，情况确实如此。保持距离或范数不变的变换很特殊，由此衍生出诸如主成分分析之类的方法。</p>
<h3 id="线性变换如何缩放区域">1.1 线性变换如何缩放区域</h3>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p140figure4.14.png" style="zoom:50%;" /></p>
<p>​ 图 4.14: 单位正方形在线性变换下的图像</p>
<p>这个平行四边形的面积描述了𝐴如何缩放单位正方形。我们暂时称之为𝜆；也就是说，</p>
<p>​ area(𝐴(𝐶)) = 𝜆 ⋅ area(𝐶),</p>
<p>其中 𝐶 = [0, 1] × [0, 1] 是单位正方形，𝐴(𝐶) 是其像</p>
<p>​ 𝐴(𝐶) ∶= {𝐴𝐱 ∶ 𝐱 ∈ 𝐶}</p>
<p>由于线性关系，𝜆 也等于任何矩形（其边与坐标轴平行）的面积与其在𝐴下像的缩放比。如图 4.15 所示，我们可以将任何平面物体近似为矩形的并集。</p>
<p>如果所有矩形都按𝜆 缩放，那么矩形的并集也会按该因子缩放。因此，𝜆 也是任何平面物体𝐸与其像𝐴(𝐸) = {𝐴𝐱 ∶ 𝐱 ∈ 𝐸} 的缩放比。</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p141figure4.15.png" style="zoom:50%;" /></p>
<p>​ 图 4.15：用矩形并集近似平面物体</p>
<p>这个量 𝜆 揭示了很多关于变换本身的信息，但还有一个问题：我们如何计算它？</p>
<p>假设我们的线性变换如下</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p141formula1.png" style="zoom:50%;" /></p>
<p>因此，它的列<span class="math inline">\(𝐱 = (𝑥_1, 𝑥_2)\)</span> 和<span class="math inline">\(𝐲 = (𝑦_1, 𝑦_2)\)</span> 描述了平行四边形的两条边。这就是单位正方形的像。</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p141figure4.16.png" style="zoom:50%;" /></p>
<p>​ 图 4.16: 单位正方形在线性变换下的图像</p>
<p>我们的面积缩放因子 𝜆 等于这个平行四边形的面积，所以我们的目标是计算出这个值。</p>
<p>任何平行四边形的面积都可以通过将底边长（在本例中为 ‖𝐱‖）乘以高<span class="math inline">\(h\)</span>来计算。（你可以很容易地通过在平行四边形的右侧切掉一个三角形并将其放在左侧，重新排列成一个矩形来看到这一点。）<span class="math inline">\(h\)</span>是未知的，但根据基本的三角学知识，我们可以看出<span class="math inline">\(ℎ = sin 𝛼‖𝐲‖\)</span>，其中 𝛼 是 𝐱 和 𝐲 之间的夹角。</p>
<p>因此， <span class="math inline">\(area=sin 𝛼‖𝐲‖‖𝐱‖\)</span></p>
<p>这几乎就是𝐱和𝐲的点积。（回想一下，点积可以写成<span class="math inline">\(⟨𝐱, 𝐲⟩ = ‖𝐱‖‖𝐲‖ cos 𝛼\)</span>。）然而，sin 𝛼部分并不匹配。</p>
<p>幸运的是，我们可以使用一个巧妙的技巧将其转换为点积！由于<span class="math inline">\(sin 𝛼 = cos (𝛼 −\frac𝜋2)\)</span>，我们得到</p>
<p>​ <span class="math inline">\(area = cos ( 𝛼 − \frac𝜋2 ) ‖𝐱‖‖𝐲‖\)</span>.</p>
<p>问题在于𝐱和𝐲之间的夹角不是<span class="math inline">\(𝛼-\frac𝜋2\)</span>。不过，我们可以通过应用旋转轻松解决这个问题（第 4.3.2 节）。应用变换</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p142formula3.png" style="zoom:50%;" /></p>
<p>我们得到 <span class="math inline">\(𝐲_{rot }= 𝑅𝐲 = (𝑦_2, −𝑦_1)\)</span></p>
<p>由于<span class="math inline">\(‖𝐲_{rot}‖ = ‖𝐲‖\)</span>，我们有</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p142formula4.png" style="zoom:50%;" /></p>
<p>仅使用矩阵 𝐴 的元素即可计算 <span class="math inline">\(⟨𝐱, 𝐲_{rot}⟩\)</span>：</p>
<p>​ <span class="math inline">\(⟨𝐱, 𝐲_{rot}⟩ = 𝑥_1𝑦_2 − 𝑥_2𝑦_1\)</span>.</p>
<p>注意，<span class="math inline">\(⟨𝐱, 𝐲_{rot}⟩\)</span> 可以为负数！当 <span class="math inline">\(𝐲 = 𝐴𝐞_2\)</span> 和 <span class="math inline">\(𝐱 = 𝐴𝐞_1\)</span> 之间的夹角（逆时针测量）大于 𝜋 时，就会发生这种情况，因为这意味着 <span class="math inline">\(cos(𝛼−\frac𝜋2 )&lt; 0\)</span>。</p>
<p>因此，<span class="math inline">\(⟨𝐱, 𝐲_{rot}⟩\)</span>这个量被称为平行四边形的有符号面积。</p>
<p>在二维中，我们称之为线性变换的行列式。也就是说，对于任何给定的线性变换/矩阵<span class="math inline">\(𝐴∈ℝ^{2×2}\)</span>，其行列式定义为</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p143formula1(4.9).png" style="zoom:50%;" /></p>
<p>行列式通常写为 |𝐴|，但我们将避免使用这种符号。我们要讨论任意矩阵 <span class="math inline">\(𝐴 ∈ ℝ^{𝑛×𝑛}\)</span> 的行列式，但为了建立直观理解，我们先暂时以 2 × 2 的情况为例。</p>
<p>行列式还揭示了向量的方向：正行列式表示正方向，负行列式表示负方向。（直观地说，正方向表示从 𝐱 到 𝐲 逆时针测量的角度在 0 到 𝜋 之间；等效地，从 𝐱 到 𝐲 顺时针测量的角度在 𝜋 到 2𝜋 之间。）如下图 4.17 所示。</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p143figure4.17.png" style="zoom:50%;" /></p>
<p>​ 图 4.17：平面中两个向量的方向</p>
<p>图片内容翻译：positively oriented: 正方向 negatively oriented:正方向</p>
<p>综上</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p143formula2(4.10).png" style="zoom:50%;" /></p>
<p>成立，其中<span class="math inline">\(𝐸⊆ℝ^2\)</span>是平面对象，并且</p>
<p>​ 𝐴(𝐸) = {𝐴𝐱 ∶ 𝐱 ∈ 𝐸}</p>
<p>是 𝐸 在变换 𝐴 下的像。</p>
<p>尽管我们只在二维中证明了 (4.10)，但这在高维情况下也是成立的。（尽管我们还不知道如何定义那里的行列式。）</p>
<p>因此，如果<span class="math inline">\(𝐞_1\)</span>和 <span class="math inline">\(𝐞_2\)</span> 是平面上的基，那么方程 (4.9) 和 (4.10) 告诉我们，二维中的行列式等于</p>
<p>​ <span class="math inline">\(det𝐴 = orientation(𝐴𝐞_1, 𝐴𝐞_2) × area(𝐴𝐞_1, 𝐴𝐞_2)\)</span></p>
<p>基于欧氏平面的例子，我们已经建立了足够的几何直觉，理解了线性变换如何扭曲体积并改变空间的方向。这些由行列式的概念描述，我们在特殊情况 (4.9) 中已经定义了它。我们将继续研究这个概念的普适性。</p>
<p>为了介绍行列式的正式定义，我们将采取一种不同于通常的思路。通常，线性变换𝐴的行列式会直接用一个复杂的公式定义，然后展示它的所有几何性质。</p>
<p>与此不同，我们将通过推广上一节中学到的几何概念来推导行列式公式。在这里，我们将大致遵循 Peter D. Lax 所著的《线性代数及其应用》的大纲。</p>
<p>我们通过介绍一些关键符号来奠定基础。设</p>
<p>​ <span class="math inline">\(𝐴 = (𝑎_{𝑖,𝑗})^𝑛_{𝑖,𝑗=1} ∈ ℝ^{𝑛×𝑛}\)</span></p>
<p>是一个列为<span class="math inline">\(𝐚_1，…，𝐚_𝑛\)</span>的矩阵。在4.1.1节中引入矩阵作为线性变换的概念时，我们看到第𝑖列是第𝑖个基向量的像。为简单起见，我们假设<span class="math inline">\(𝐞_1，𝐞_2，…，𝐞_𝑛\)</span>是标准正交基，也就是说，<span class="math inline">\(𝐞_𝑖\)</span>是第𝑖个坐标为1，其余坐标为0的向量。因此，<span class="math inline">\(𝐴𝐞_𝑖 = 𝐚_𝑖\)</span>。</p>
<p>在4.3节对欧氏平面的探索中，我们已经看到，行列式是基向量像的方向乘以它们所定义的平行四边形的面积。按照这个逻辑，我们可以定义𝑛 × 𝑛 矩阵的行列式为</p>
<p>​ <span class="math inline">\(det𝐴 = orientation(𝐴𝐞_1, … , 𝐴𝐞_𝑛) × volume(𝐴𝐞_1, … , 𝐴𝐞_𝑛)\)</span></p>
<p>两个问题立刻浮现出来。首先，我们如何定义𝑛维空间中多个向量的方向？其次，我们如何计算面积？</p>
<p>我们不会直接寻找这些问题的答案，而是要给故事增添一个转折：首先，我们会找到一个方便的行列式公式，然后用它来定义方向。</p>
<h3 id="行列式的多重线性">1.2 行列式的多重线性</h3>
<p>为了更明确地表达行列式与矩阵<span class="math inline">\(𝐚_𝑖 = 𝐴𝐞_𝑖\)</span>的列之间的关系，我们写成：</p>
<p>​ <span class="math inline">\(det𝐴 = det(𝐚_1, … , 𝐚_𝑛)\)</span>.</p>
<p>以这种方式思考行列式，det 只是多个变量的函数：</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p145formula2.png" style="zoom:50%;" /></p>
<p>好消息：$det(𝐚_1, … , 𝐚_𝑛) $在每个变量上都是线性的。也就是说，</p>
<p>​ <span class="math inline">\(det(𝐚_1, … , 𝛼𝐚_𝑖 + 𝛽𝐛_𝑖 , … 𝐚_𝑛) = 𝛼 det(𝐚_1, … , 𝐚_𝑖 , … 𝐚_𝑛) + 𝛽 det(𝐚_1, … , 𝐛_𝑖 , … 𝐚_𝑛)\)</span></p>
<p>成立。我们不打算证明这一点，但由于行列式表示有符号的体积，你可以通过查看图 4.18 来验证。</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p145figure4.18.png" style="zoom:50%;" /></p>
<p>​ 图 4.18: <span class="math inline">\(det(𝐚_𝟏, 𝐚_𝟐)\)</span> 的多重线性</p>
<p>线性的一个结果是，我们可以将行列式表示为标准基向量<span class="math inline">\(𝐞_1, …,𝐞_𝑛\)</span>的行列式的线性组合。例如，考虑以下内容。由于</p>
<p>​ <span class="math inline">\(𝐴𝐞_1 = 𝐚_1 =∑^𝑛_{𝑖=1}𝑎_{𝑖,1}𝐞_𝑖 ,\)</span></p>
<p>我们有</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p146formula1.png" style="zoom:50%;" /></p>
<p>更进一步，利用这一点</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p146formula2.png" style="zoom:50%;" /></p>
<p>我们开始注意到一种模式。有了线性，我们</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p146formula3(4.11).png" style="zoom:50%;" /></p>
<p>我们可以看到，系数<span class="math inline">\(𝑎_{𝑖,1}𝑎_{𝑗,2}\)</span> 中的行索引与 <span class="math inline">\(det(𝐞_𝑖, 𝐞_𝑗, 𝐚_3, … , 𝐚_𝑛)\)</span> 中<span class="math inline">\(𝐞_𝑘\)</span>-s 的索引相匹配。一般情况下，这种模式可以用排列形式化表示；即集合 {1, 2, … , 𝑛} 的排序。</p>
<p>你可以将排列想象成一个函数𝜎，它将 {1, 2, … , 𝑛} 映射到自身，使得对于每个𝑗 ∈{1, 2, … , 𝑛}，都存在一个𝑖 ∈ {1, 2, … , 𝑛}，且𝜎(𝑖) = 𝑗。换句话说，你取 1 到 𝑛 之间的每个整数，并将它们按顺序排列。{1, 2, … , 𝑛} 上所有可能的排列的集合记为<span class="math inline">\(𝑆_𝑛\)</span>。</p>
<p>继续 (4.11) 并进一步展开𝐴的行列式，我们得到</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p146formula4.png" style="zoom:50%;" /></p>
<p>这个公式不太容易理解。你可以把每个项 <span class="math inline">\(∏^𝑛_{𝑖=1} 𝐚_{𝜎(𝑖),𝑖}\)</span> 想象成将 𝑛 个国际象棋车放在 𝑛 × 𝑛 的棋盘上，使得它们之间无法互相捕获。</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p146figure4.19.png" style="zoom:50%;" /></p>
<p>​ 图 4.19: 项 <span class="math inline">\(𝐚_{𝜎(1)1} ⋯ 𝐚_{𝜎(𝑛)n}\)</span> 的解析结构</p>
<p>公式</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p147formula1.png" style="zoom:50%;" /></p>
<p>结合了所有可能的方法。</p>
<p>只剩下一件事：计算 <span class="math inline">\(det(𝐞_{𝜎(1)}, … , 𝐞_{𝜎(𝑛)})\)</span>。</p>
<p>还记得我们讨论过欧氏平面中反射和旋转的组合吗（第 4.3.4 节）？由$ 𝐞_𝑖 ↦ 𝐞_{𝜎(𝑖)}$ 确定的变换与此类似。谈到排列时，最好知道，每个排列都可以通过一次交换两个元素来获得。一个排列中，转置次数（即影响两个元素的排列）称为 sign(𝜎)。在我们的线性变换<span class="math inline">\(𝐞_𝑖 ↦𝐞_{𝜎(𝑖)}\)</span>中，𝜎中的转置次数就是所需的反射次数，而sign(𝜎)是<span class="math inline">\((𝐞_{𝜎(1)}, … ,𝐞_{𝜎(𝑛)})\)</span>的方向。</p>
<p>至此，有了这些，我们最终可以给出行列式和方向的正式定义。</p>
<blockquote>
<p>定义 4.4.1（行列式和方向）</p>
<p>设 𝐴 ∈ ℝ^{𝑛×𝑛} 为任意矩阵，<span class="math inline">\(𝐚_𝑖 ∈ ℝ^𝑛\)</span> 为其第 𝑖 列。𝐴 的行列式定义为</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p147formula2(4.12).png" style="zoom:50%;" /></p>
<p>向量<span class="math inline">\(𝐚_1, … , 𝐚_𝑛\)</span>的方向为 <span class="math inline">\(orientation(𝐚_1, … , 𝐚_𝑛) ∶= sign(det𝐴)\)</span></p>
</blockquote>
<p>当行列式表示法不方便时，我们会将矩阵元素放在一个很大的绝对值符号内来表示行列式：</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p147formula4.png" style="zoom:50%;" /></p>
<p>当我还是一名年轻的数学学生时，行列式公式 (4.12) 就在我的第一堂线性代数课上被原封不动地呈现了出来。由于没有解释它与体积和方向的联系，我花了好几年才真正理解它。我仍然认为行列式是线性代数中最复杂的概念之一，尤其是在没有几何学依据的情况下。</p>
<p>现在你已经对行列式有了基本的了解，你可能会问：我们该如何在实际中计算它？对所有排列的集合求和并计算它们的符号，从计算的角度来看并非易事。</p>
<p>好消息是：行列式有一个递归公式。坏消息是：对于一个 𝑛 × 𝑛 矩阵，它涉及 𝑛 个 (𝑛 − 1) × (𝑛 − 1) 矩阵。尽管如此，这与排列公式相比还已经好了很多。让我们来看一下！</p>
<blockquote>
<p>定理 4.4.1（行列式的递归公式）</p>
<p>设 <span class="math inline">\(𝐴 ∈ ℝ^{𝑛×𝑛}\)</span>为任意方阵。则</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p148formula1(4.13).png" style="zoom:50%;" /></p>
<p>其中 <span class="math inline">\(𝐴_{𝑖,𝑗}\)</span> 是通过从 𝐴 中移除第 𝑖 行和第 𝑗 列得到的 (𝑛 − 1) × (𝑛 − 1) 矩阵。</p>
</blockquote>
<p>我们不会给出证明，而是提供一个例子来演示这个公式。对于 3 × 3 矩阵，它的形式如下：</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p148formula2.png" style="zoom:50%;" /></p>
<p>现在我们既掌握了几何直觉，又掌握了递归公式，让我们来看看行列式最重要的性质！</p>
<h3 id="行列式的基本性质">1.3 行列式的基本性质</h3>
<p>在处理行列式时，我们倾向于创建基本的构建块和组合规则。（正如我们在推导(4.12)时多次看到的，这种模式也适用。）这些规则体现在行列式的基本性质中，我们现在将对此进行讨论。</p>
<p>第一个性质与组合和行列式的关系有关。</p>
<blockquote>
<p>定理 4.4.2（行列式的乘积）</p>
<p><span class="math inline">\(A,B∈ℝ^{𝑛×𝑛}\)</span>是两个矩阵。则</p>
<p>​ <span class="math inline">\(det𝐴𝐵 = det𝐴det 𝐵\)</span> (4.14)</p>
</blockquote>
<p>方程 (4.14) 被称为行列式乘积法则，其证明需要基于公式 (4.12) 和 (4.13) 进行大量计算。我不会提供完整的证明，而是给出一个直观的解释。毕竟，我们想要用数学来构建算法，而不是构建数学。</p>
<p>因此，det𝐴𝐵 = det𝐴det 𝐵 的解释非常简单。如果我们将矩阵 <span class="math inline">\(𝐴, 𝐵 ∈ ℝ^{𝑛×𝑛}\)</span>视为线性变换，我们刚刚看到 det𝐴 和 det 𝐵 决定了它们如何缩放单位立方体。</p>
<p>由于这些线性变换的复合是矩阵乘积𝐴𝐵，线性变换𝐴𝐵将单位立方体缩放为一个平行六面体，其有符号的体积为det𝐴det𝐵。（因为应用𝐴𝐵等同于先应用𝐵，然后再对结果应用𝐴。）</p>
<p>因此，根据我们对行列式的理解，由于𝐴𝐵的缩放因子也是 det𝐴𝐵，所以（4.14）成立。</p>
<p>我们可以对此进行实际证明，例如，基于递归公式（4.13）进行归纳法，这将导致一个漫长而复杂的计算。</p>
<p>乘积法则的一个直接推论是矩阵的行列式与其逆矩阵之间存在一种特殊的关系。</p>
<blockquote>
<p>定理 4.4.3 设 <span class="math inline">\(𝐴 ∈ ℝ^{𝑛×𝑛}\)</span>为任意可逆矩阵。则</p>
<p><span class="math inline">\(det𝐴^{−1} = (det𝐴)^{−1}\)</span></p>
</blockquote>
<blockquote>
<p>证明。利用乘积法则，我们有 <span class="math inline">\(1 = det 𝐼 = det𝐴𝐴^{−1} = (det𝐴)(det𝐴^{−1})\)</span>， 由此可知定理成立。</p>
</blockquote>
<p>正因为如此，我们还可以得出结论，行列式由相似关系得到。</p>
<blockquote>
<p>定理 4.4.4 设 <span class="math inline">\(𝐴, 𝐵 ∈ ℝ^{𝑛×𝑛}\)</span>为两个相似矩阵，且 <span class="math inline">\(𝐵 = 𝑇^{−1}𝐴𝑇\)</span>，其中 <span class="math inline">\(𝑇 ∈ ℝ^{𝑛×𝑛}\)</span>。则 <span class="math inline">\(det𝐴 = det 𝐵\)</span>。</p>
</blockquote>
<blockquote>
<p>证明。这简单地从</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p150formula1.png" style="zoom:50%;" /></p>
<p>中得到，这个证明我必须要展示在这里的。</p>
</blockquote>
<p>另一个重要结果是，行列式与矩阵所在的基无关。如果<span class="math inline">\(𝐴 ∶ 𝑈 → 𝑈\)</span> 是线性变换，<span class="math inline">\(𝑃 = \{𝐩_1, … , 𝐩_𝑛\}\)</span> 和 <span class="math inline">\(𝑅 = \{𝐫_1, … , 𝐫_𝑛\}\)</span> 是 𝑈 的两个基，那么我们知道，该变换的矩阵通过以下关系相互关联（见第 4.2.1 节）：</p>
<p>​ $𝐴_𝑃 = 𝑇^{−1}𝐴_𝑅𝑇 $</p>
<p>其中<span class="math inline">\(𝐴_𝑆\)</span>是变换<span class="math inline">\(𝐴\)</span>在基<span class="math inline">\(𝑆\)</span>上的矩阵，<span class="math inline">\(𝑇 ∈ ℝ^{𝑛×𝑛}\)</span>是基变换矩阵（4.2.1节）。前定理表明<span class="math inline">\(det𝐴_𝑃 = det𝐴_𝑅\)</span>。因此，行列式的定义不仅适用于矩阵，也适用于线性变换！</p>
<p>行列式本质上存在一个对偶关系：你可以在交换矩阵的行和列的同时保持所有与行列式相关的恒等式成立。</p>
<blockquote>
<p>定理 4.4.5 设$ 𝐴 ∈ ℝ^{𝑛×𝑛}$为任意矩阵。则 <span class="math inline">\(det𝐴 = det𝐴^𝑇\)</span></p>
</blockquote>
<blockquote>
<p>证明。设<span class="math inline">\(𝐴 = (𝑎_{𝑖,𝑗})^𝑛_{𝑖,𝑗=1}\)</span>。设其转置矩阵的元素为<span class="math inline">\(𝑎^𝑡_{𝑖,𝑗} = 𝑎_{𝑗,𝑖}\)</span>。根据 (4.12)，我们有</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p150formula4.png" style="zoom:50%;" /></p>
<p>这里要注意，由于乘积 <span class="math inline">\(∏^𝑛_{𝑖=1} 𝑎_{𝑖,𝜎(𝑖)}\)</span>会遍历所有𝑖，并且项的顺序无关紧要，所以我们不妨将项的顺序设置为 <span class="math inline">\(𝑖=𝜎^{−1}(1),…,𝜎^{−1}(𝑛)\)</span>。因为</p>
<p>​ <span class="math inline">\(sign(𝜎^{−1}) =sign(𝜎)\)</span></p>
<p>继续上述计算，我们得到</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p151formula1.png" style="zoom:40%;" /></p>
<p>因为每个排列都是可逆的，并且 <span class="math inline">\(𝜎 ↦ 𝜎^{-1}\)</span> 是双射，所以对 <span class="math inline">\(𝜎 ∈ 𝑆_𝑛\)</span> 求和与对 <span class="math inline">\(𝜎^{-1} ∈ 𝑆_𝑛\)</span> 求和相同。</p>
<p>结合以上所有内容，我们得到</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p151formula2.png" style="zoom:50%;" /></p>
<p>这是我们必须要知道的。</p>
</blockquote>
<blockquote>
<p>定理 4.4.6 设 <span class="math inline">\(𝐴 ∈ ℝ^{𝑛×𝑛}\)</span>为任意矩阵，<span class="math inline">\(𝐴^{𝑖,𝑗}\)</span> 表示通过交换 𝐴 第 𝑖 列和第 𝑗 列得到的矩阵。则</p>
<p>​ <span class="math inline">\(det𝐴^{𝑖,𝑗} = − det𝐴\)</span>,</p>
<p>或者换句话说，交换𝐴的任意两列都会改变行列式的符号。同样，交换两行也会改变行列式的符号。</p>
</blockquote>
<blockquote>
<p>证明。这源于 (4.14) 的巧妙应用，注意到 <span class="math inline">\(𝐴^{𝑖,𝑗} = 𝐴𝐼^{𝑖,𝑗}\)</span>，其中 <span class="math inline">\(𝐼^{𝑖,𝑗}\)</span> 是通过交换单位矩阵的第 𝑖 列和第 𝑗 列得到的。<span class="math inline">\(det 𝐼^{𝑖,𝑗}\)</span> 是行列式，形式为 <span class="math inline">\(det(𝐞_{𝜎(1)}, … , 𝐞_{𝜎(𝑛)})\)</span>，其中 𝜎 是简单地交换 𝑖 和 𝑗 的排列。（也就是说，𝜎 是转置。）因此，</p>
<p>​ <span class="math inline">\(det𝐴^{𝑖,𝑗} = det𝐴det𝐼^{𝑖,𝑗} = − det𝐴\)</span></p>
<p>这就是我们要证明的。</p>
<p>关于交换行，我们可以应用之前的结果，因为转置矩阵保留了行列式。</p>
</blockquote>
<p>因此，具有两个相同行的矩阵具有零行列式。</p>
<blockquote>
<p>定理 4.4.7 设 <span class="math inline">\(𝐴 ∈ ℝ^{𝑛×𝑛}\)</span>为具有两个相同行或列的矩阵。则 det𝐴 = 0。</p>
</blockquote>
<blockquote>
<p>证明：假设第𝑖列和第𝑗列相同。由于两列相等，<span class="math inline">\(det𝐴^{𝑖,𝑗} =det𝐴\)</span>。然而，应用前面的定理（该定理指出交换两列会改变行列式的符号），我们得到 <span class="math inline">\(det𝐴^{𝑖,𝑗} = − det𝐴\)</span>。只有当 det𝐴 = 0 时，这才成立。</p>
<p>同样，转置这个矩阵可以证明矩阵的两个行相同时的情况。</p>
</blockquote>
<p>另一个结果是，我们获得了线性相关向量系统和行列式之间的本质联系。</p>
<blockquote>
<p>定理 4.4.8 设 <span class="math inline">\(𝐴 ∈ ℝ^{𝑛×𝑛}\)</span>为一个矩阵。则其列线性相关当且仅当 det𝐴 = 0。类似地，𝐴 的行线性相关当且仅当 det𝐴 = 0。</p>
</blockquote>
<blockquote>
<p>证明。(i) 首先，我们将证明线性相关的列（或行）意味着 det𝐴 = 0。照例，我们将𝐴的列表示为<span class="math inline">\(𝐚_1, … , 𝐚_𝑛\)</span>，为了简单起见，假设</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p152formula1.png" style="zoom:50%;" /></p>
<p>由于行列式是列的线性函数，因此我们有</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p152formula2.png" style="zoom:50%;" /></p>
<p>根据上一定理，所有项 $det(𝐚_1, 𝐚+2, … , 𝐚_𝑛) $均为零，这意味着 det𝐴 = 0，这正是我们要证明的。如果行是线性相关的，我们应用上述定理可得 <span class="math inline">\(det𝐴 = det𝐴^𝑇 = 0\)</span>。 (ii) 现在，我们来证明 det𝐴 = 0 表示列是线性相关的。与其给出相当复杂的精确证明，不如给出一个直观的解释。</p>
<p>回想一下，行列式是方向乘以由列给出的平行六面体的体积。由于方向为 ±1，det𝐴 意味着平行六面体的体积为 0。这只有当 𝑛列位于 𝑛 − 1 维子空间中时才会发生，这意味着它们是线性相关的。</p>
</blockquote>
<p>我们可以立即应用它来获得以下结果。</p>
<blockquote>
<p>推论 4.4.1 设 <span class="math inline">\(𝐴 ∈ ℝ^{𝑛×𝑛}\)</span>为具有常数零列（或行）的矩阵。则 det𝐴 = 0。</p>
</blockquote>
<p>由于行列式是基向量像的有符号体积，因此在某些情况下它可以为零。这些变换相当特殊。什么时候会发生这种情况？让我们回到欧几里德平面来建立一些直观的理解。</p>
<p>在那里，我们有</p>
<p><img src="/Users/adline/翻译/机器学习的数学基础/pics2/p153formula1.png" style="zoom:50%;" /></p>
<p>或者换句话说，<span class="math inline">\(\frac{𝑥_1}{𝑦_1}=\frac{𝑥_2}{𝑦_2}\)</span>。对此还有一种解释：向量 <span class="math inline">\((𝑦_1, 𝑦_2)\)</span> 是<span class="math inline">\((𝑥_1, 𝑥_2)\)</span> 的标量倍数；也就是说，它们是共线的，这意味着它们位于通过原点的同一条直线上。从线性变换的角度思考，这意味着<span class="math inline">\(𝐞_1\)</span> 和<span class="math inline">\(𝐞_2\)</span> 的像位于<span class="math inline">\(ℝ^2\)</span> 的一个子空间上。正如我们接下来会看到的，这与变换的可逆性密切相关。</p>
<blockquote>
<p>定理 4.4.9（可逆性和行列式） 线性变换 <span class="math inline">\(𝐴 ∈ ℝ^{𝑛×𝑛}\)</span>可逆当且仅当 det𝐴 ≠ 0。</p>
</blockquote>
<blockquote>
<p>证明。当我们引入可逆性的概念（定义 4.1.2）时，我们发现𝐴可逆当且仅当它的列<span class="math inline">\(𝐚_1, …,𝐚_𝑛\)</span>构成一个基。因此，它们是线性独立的。 由于列的线性独立性（定义 1.2.1）等价于非零行列式，因此结论呼之欲出。</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2(%E4%B8%89)/" rel="prev" title="机器学习的数学基础-线性变换(三)">
      <i class="fa fa-chevron-left"></i> 机器学习的数学基础-线性变换(三)
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/11/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2(%E4%BA%94)/" rel="next" title="机器学习的数学基础-线性变换(五)">
      机器学习的数学基础-线性变换(五) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>

  
  
  




          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#行列式线性变换如何影响体积"><span class="nav-number">1.</span> <span class="nav-text">1 行列式，线性变换如何影响体积</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性变换如何缩放区域"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 线性变换如何缩放区域</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#行列式的多重线性"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 行列式的多重线性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#行列式的基本性质"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 行列式的基本性质</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Adline125"
      src="/images/icon.png">
  <p class="site-author-name" itemprop="name">Adline125</p>
  <div class="site-description" itemprop="description">北京程序媛一枚，主攻NLP.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Adline125" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Adline125" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:gglfxsld@gmail.com" title="E-Mail → mailto:gglfxsld@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Adline125</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>
<br/>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次 | 本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  var disqus_config = function() {
    this.page.url = "http://yoursite.com/2025/11/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2(%E5%9B%9B)/";
    this.page.identifier = "2025/11/01/机器学习的数学基础-线性变换(四)/";
    this.page.title = "机器学习的数学基础-线性变换(四)";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://Adline125.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
