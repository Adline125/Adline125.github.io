<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文是对XGBoost的经典论文XGBoost: A Scalable Tree Boosting System的总结。XGBoost 的全称是 eXtreme Gradient Boosting。作者为华盛顿大学研究机器学习的大牛陈天奇，其最大的特点在于，它能够自动利用 CPU 的多线程进行并行，同时在算法上加以改进提高了精度。 本文更多关注算法，主要内容如下：">
<meta property="og:type" content="article">
<meta property="og:title" content="XGBoost: A Scalable Tree Boosting System">
<meta property="og:url" content="http://yoursite.com/2020/06/01/XGBoost-A-Scalable-Tree-Boosting-System/index.html">
<meta property="og:site_name" content="Adline125&#39;s Blog">
<meta property="og:description" content="本文是对XGBoost的经典论文XGBoost: A Scalable Tree Boosting System的总结。XGBoost 的全称是 eXtreme Gradient Boosting。作者为华盛顿大学研究机器学习的大牛陈天奇，其最大的特点在于，它能够自动利用 CPU 的多线程进行并行，同时在算法上加以改进提高了精度。 本文更多关注算法，主要内容如下：">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-06-01T06:55:26.000Z">
<meta property="article:modified_time" content="2020-07-04T01:49:38.500Z">
<meta property="article:author" content="Adline125">
<meta property="article:tag" content="Machine leanring">
<meta property="article:tag" content="XGBoost">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2020/06/01/XGBoost-A-Scalable-Tree-Boosting-System/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>XGBoost: A Scalable Tree Boosting System | Adline125's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Adline125's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Adline125's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">NLP Engineer, Google Developers Expert</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/01/XGBoost-A-Scalable-Tree-Boosting-System/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/icon.png">
      <meta itemprop="name" content="Adline125">
      <meta itemprop="description" content="北京程序媛一枚，主攻NLP.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Adline125's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          XGBoost: A Scalable Tree Boosting System
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-01 14:55:26" itemprop="dateCreated datePublished" datetime="2020-06-01T14:55:26+08:00">2020-06-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-04 09:49:38" itemprop="dateModified" datetime="2020-07-04T09:49:38+08:00">2020-07-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文是对XGBoost的经典论文<a href="https://dl.acm.org/doi/pdf/10.1145/2939672.2939785" target="_blank" rel="noopener">XGBoost: A Scalable Tree Boosting System</a>的总结。<a href="https://github.com/dmlc/xgboost" target="_blank" rel="noopener">XGBoost</a> 的全称是 eXtreme Gradient Boosting。作者为华盛顿大学研究机器学习的大牛<a href="http://homes.cs.washington.edu/~tqchen/" target="_blank" rel="noopener">陈天奇</a>，其最大的特点在于，它能够自动利用 CPU 的多线程进行并行，同时在算法上加以改进提高了精度。</p>
<p>本文更多关注算法，主要内容如下：</p>
<a id="more"></a>
<ul>
<li>XGBoost目标函数推导</li>
<li>XGBoost防止过拟合策略</li>
<li>切分搜索算法</li>
<li>XGBoost针对稀疏问题的处理</li>
<li>系统速度优化</li>
</ul>
<h3 id="xgboost目标函数推导">XGboost目标函数推导</h3>
<hr />
<h4 id="预测值">预测值</h4>
<p>XGboost是基于回归树的集成学习模型。集成学习主要有两种方式：bagging和boosting。XGboost属于boosting, 其预测值通过对各弱分类器的结果求和得到。对于输入<span class="math inline">\(x_{i}\)</span>： <span class="math display">\[
\begin{split}\hat { y } _ { i } = \phi ( x _ { i } ) = \sum _ { k = 1 } ^ { K } f _ { k } ( x _ { i } ) , \quad f _ { k } \in F  \qquad (1)
\end{split}
\]</span> 其中，<span class="math inline">\(f _ {k}\)</span>即为一个弱分类器，在XGboost中，弱分类器为 CART。<em>F</em> 为 CART 空间。</p>
<h4 id="目标函数">目标函数</h4>
<p>为了学习得到上式中的K个<span class="math inline">\(f_{k}\)</span>, 我们需要通过最小化模型的目标函数来获得。目标函数通常由损失+正则项表示。损失指示离真实值的差距，正则防止过拟合。这里损失为<span class="math inline">\(\begin{split}\sum _{ i } l ( \hat y _ { i }, y)\end{split}\)</span>, 是个可微凸函数。正则项则是K个<span class="math inline">\(f _ {k}\)</span>中的参数。由于XGBoost的<span class="math inline">\(f _ {k}\)</span>是回归树，该目标函数如下：</p>
<p><span class="math display">\[
\begin{split}&amp;O b j = \sum _ { i } l ( \hat { y } _ { i } , y _ { i } ) + \sum _ { k } \Omega ( f _ { k } ) \\
&amp;where\quad\Omega ( f ) = \gamma T + \frac { 1 } { 2 } \lambda | w | ^ { 2 } \nonumber \end{split} \qquad (2)
\]</span> 其中，$( f ) $ 是惩罚项。<span class="math inline">\(f_{ k }\)</span>做为回归树，影响其复杂度的参数为叶子节点的个数<span class="math inline">\(\, T \,\)</span>及叶子节点的权重<span class="math inline">\(w\)</span>.</p>
<h4 id="训练方法">训练方法</h4>
<p>由于<span class="math inline">\(f _ { k }\)</span>是树，与其他模型<span class="math inline">\(f\)</span> 可以由数值型向量表示不同，我们不能使用往常的优化算法，如SGD，来寻找<span class="math inline">\(f\)</span>。这里的解决方案是Additive Training（Boosting）。</p>
<p><span class="math display">\[
\begin{split} &amp;\hat { y } _ { i } ^ { ( 0 ) } = 0  \\ 
&amp;\hat { y } _ { i } ^ { ( 1 ) } = f _ { 1 } ( x _ { i } ) = \hat { y } _ { i } ^ { ( 0 ) } + f _ { 1 } ( x _ { i } )  \\
&amp;\hat { y } _ { i } ^ { ( 2 ) } = f _ { 1 } ( x _ { i } ) + f _ { 2 } ( x _ { i } ) = \hat { y } _ { i } ^ { ( 1 ) } + f _ { 2 } ( x _ { i } )  \\
&amp; …  \\ 
&amp;\hat { y } _ { i } ^ { ( t ) } = \sum _ { k = 1 } ^ { t } f _ { k } ( x _ { i } ) = \hat { y } _ { i } ^ { ( t - 1 ) } + f _ { t } ( x _ { i } )  
\end{split}
\]</span> 其中，每一轮添加的<span class="math inline">\(f\)</span> 是最优化目标函数的结果。</p>
<h4 id="目标函数近似形式">目标函数近似形式</h4>
<p>根据Additive Training方式，在第<span class="math inline">\(t\)</span> 轮，<span class="math inline">\(\hat { y } _ { i } ^ { ( t ) } = \hat { y } _ { i } ^ { ( t - 1 ) } + f _ { t } ( x _ { i } )\)</span>。 由此在第<span class="math inline">\(t\)</span>轮的目标函数可以改写为：</p>
<p><span class="math display">\[
\begin{split}
O b j ^ { ( t ) } &amp;= \sum _ { i = 1 } ^ { n } l ( y _ { i } , \hat { y } _ { i } ^ { ( t ) } ) + \sum _ { i = 1 } ^ { t } \Omega ( f _ { i } )  \\ 
&amp;= \sum _ { i = 1 } ^ { n } l ( y _ { i } , \hat { y } _ { i } ^ { ( t - 1 ) } + f _ { t } ( x _ { i } ) ) + \Omega ( f _ { t } ) + constant 
\end{split}
\]</span> 这个目标函数仍然很难求最优。我们用泰勒展开取其近似如下：</p>
<p><span class="math display">\[
O b j ^ { ( t ) } \simeq \sum _ { i = 1 } ^ { n } [ l ( y _ { i } , \hat { y } _ { i } ^ { ( t - 1 ) } ) + g _ { i } f _ { t } ( x _ { i } ) + \frac { 1 } { 2 } h _ { i } f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) + constant
\]</span> 其中，<span class="math inline">\(g _ { i } = \partial _ { \hat { y } ^ { ( t - 1 ) } } l ( y _ { i } , \hat { y } ^ { ( t - 1 ) } ) , \quad h _ { i } = \partial _ { \hat { y } ^ { ( t - 1 ) } } ^ { 2 } l ( y _ { i } , \hat { y } ^ { ( t - 1 ) } )\)</span>, <span class="math inline">\(g _ {i}\)</span>, <span class="math inline">\(h _ {i}\)</span>分别为 <span class="math inline">\(l ( y _ { i } , \hat { y } _ { i } ^ { ( t - 1 ) } )\)</span>对<span class="math inline">\(\hat y ^ {(t - 1)}\)</span>求偏导的一阶和二阶导数。把上式中的常数项去掉，得：</p>
<p><span class="math display">\[
\tilde {  O b j ^ { ( t ) } } = \sum _ { i = 1 } ^ { n } [ g _ { i } f _ { t } ( x _ { i } ) + \frac { 1 } { 2 } h _ { i } f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) \qquad (3)
\]</span> 这里要注意理解上式是针对每一个样本<span class="math inline">\(x_{i}\)</span>求损失，然后求所有样本损失和。我们将要在接下来的计算中为了方便计算<span class="math inline">\(\tilde { O b j ^ { ( t ) } }\)</span>的最优值而换一种对这波操作的形式化表达方式。</p>
<h4 id="目标函数求最优">目标函数求最优</h4>
<p>这里定义 <span class="math inline">\(I _ { j } = \{ i | q ( x _ { i } ) = j \}\)</span>。<span class="math inline">\(I_{j}\)</span>为被树分配到叶子节点<span class="math inline">\(j\)</span> 上的样本集。<span class="math inline">\(q ( x _ { i } )\)</span>为样本<span class="math inline">\(x _ {i}\)</span>通过q这个树结构到达叶子节点<span class="math inline">\(j\)</span>。 <span class="math inline">\(w_ {j}\)</span>为第<span class="math inline">\(j\)</span>个叶子节点的权重。则 <span class="math inline">\(f _ { t } ( x _ { i } ) = w _ { q ( x _ { i } ) }\)</span>。因此式 <span class="math inline">\((3)\)</span> 可以表示为： <span class="math display">\[
\begin{split}
O b j ^ { ( t ) } &amp;\simeq \sum _ { i = 1 } ^ { n } [ g _ { i } f _ { t } ( x _ { i } ) + \frac { 1 } { 2 } h _ { i } f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) \\ 
&amp;= \sum _ { i = 1 } ^ { n } [ g _ { i } w _ { q ( x _ { i } ) } + \frac { 1 } { 2 } h _ { i } w _ { q ( x _ { i } ) } ^ { 2 } ] + \gamma T + \lambda \frac { 1 } { 2 } \sum _ { j = 1 } ^ { T } w _ { j } ^ { 2 } \\ 
&amp;= \sum _ { j = 1 } ^ { T } [ ( \sum _ { i \in I _ { j } } g _ { i } ) w _ { j } + \frac { 1 } { 2 } ( \sum _ { i \in I _ { j } } h _ { i } + \lambda ) w _ { j } ^ { 2 } ] + \gamma T
\end{split}
\]</span> 中括号里为我们求最优解时最喜欢形式<span class="math inline">\(ax ^ 2 + bx\)</span>。显然，当</p>
<p><span class="math display">\[
w _ { j } ^ { * } = - \frac { \sum _ { i \in I _ { j } } g _ { i } } { \sum _ { i \in I _ { j } } h _ { i } + \lambda }
\]</span> 时，得到最优的<span class="math inline">\(\tilde {O b j ^ { ( t ) }}\)</span>:</p>
<p><span class="math display">\[
\tilde { Obj } ^ { ( t ) }  = - \frac { 1 } { 2 } \sum _ { j = 1 } ^ { T } \frac { ( \sum _ { i \in I _ { j } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { j } } h _ { i } + \lambda } + \gamma T \qquad (4)
\]</span> 至此，我们完成了求第<span class="math inline">\(t\)</span>轮时目标函数的最优值。这个值可以用来衡量树结构的好坏。也就是说，判断一棵树的质量，我们只要算出<span class="math inline">\(g _ {i}\)</span>, <span class="math inline">\(h_{i}\)</span>，然后代入式子<span class="math inline">\((4)\)</span>即可算出。</p>
<h3 id="防止过拟合策略">防止过拟合策略</h3>
<hr />
<ul>
<li>权重衰减（Shrinkage）: 在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型。</li>
<li>列采样（Column subsampling）: 同对数据集做有放回的采样可以防止拟合一样，对特征做有放回的采样也可以防止过拟合。有两种方式：建树前采样和节点分裂之前采样。不仅如此，此方法还可以加快模型计算速度。</li>
</ul>
<h3 id="切分搜索算法">切分搜索算法</h3>
<hr />
<p>当我们把一个节点做了切分后，我们需要一个评估这个切分是好是坏的标准。一个朴素的思想就是切分前节点的目标函数的得分<strong>减去</strong>切分后左右两节点目标函数之和的得分，如果两者的差距较大，说明切分后的损失较小。计算公式如下： <span class="math display">\[
\begin{split}
L _ { s p l i t } &amp;= L ^ { ( t ) } - ( L _ { l } ^ { ( t ) } + L _ { r } ^ { ( t ) } ) \\
&amp;= - \frac { 1 } { 2 } \frac { ( \sum _ { i \in I _ { j } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { j } } h _ { i } + \lambda } + \gamma - ( - \frac { 1 } { 2 } \frac { ( \sum _ { i \in I _ { I } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { l } } h _ { i } + \lambda } + - \frac { 1 } { 2 } \frac { ( \sum _ { i \in I _ { r } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { r } } h _ { i } + \lambda } + 2 \gamma )\\
&amp;= \frac { 1 } { 2 } [ \frac { ( \sum _ { i \in I _ { L } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { L } } h _ { i } + \lambda } + \frac { ( \sum _ { i \in I _ { R } } g _ { i } ) ^ { 2 } } { \sum _ { i \in I _ { R } } h _ { i } + \lambda } - \frac { ( \sum _ { i \in I } g _ { i } ) ^ { 2 } } { \sum _ { i \in I } h _ { i } + \lambda } ] - \gamma
\end{split} \qquad (5)
\]</span></p>
<h4 id="贪心算法">贪心算法</h4>
<p>遍历所有特征的所有可能切分点，计算（5）式，取得分最高的切分点进行切分。</p>
<h4 id="近似算法">近似算法</h4>
<p>近似算法按特征分布的百分位提出候选切分点，相当于将特征的特征值分桶后再遍历。按提出候选切分点的时间不同，有两种切分的方式：全局切分和局部切分。</p>
<p>全局切分就是在建树之初就提出了每个特征的候选切分点，之后对树的所有节点做切分时都从这些节点对应的特征候选切分点中选择。</p>
<p>局部切分是在每次节点切分后优化提出候选切分点。候选切分点提出较全局切分频繁，较全局切分需要较少切分点。适合生成较深的树。</p>
<p>全局切分候选切分点足够多时，可以达到和局部切分差不多的效果。</p>
<h4 id="加权分位点">加权分位点</h4>
<p>加权分位点是近似算法的一种。上面提到的近似算法通常是将切分点设在使数据均匀分布的位置。而加权分位点则是将切分点设在使分桶后的数据对loss产生均衡影响的位置。如果分4个桶，加权分位点使得每个桶内的样本集对loss的影响持平为25%。</p>
<p>样本对loss的影响计算方法如下： <span class="math display">\[
\begin{split} 
L ^ { ( t ) } &amp;\simeq \sum _ { i = 1 }^ { n } [ l ( y _ { i } , \hat { y } _ { i } ^ { ( t - 1 ) } ) + g _ { i } f _ { t } ( x _ { i } ) + \frac { 1 } { 2 } h _ { i } f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) + constant \\
&amp;= \sum _ { i = 1 } ^ { n } [ \frac { 1 } { 2 } h _ { i } \cdot \frac { 2 \cdot g _ { i } f _ { t } ( x _ { i } ) } { h _ { i } } + \frac { 1 } { 2 } h _ { i } \cdot f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) + constant \\
&amp;= \sum _ { i = 1 } ^ { n } \frac { 1 } { 2 } h _ { i } [ 2 \cdot \frac { g _ { i } } { h _ { i } } \cdot f _ { t } ( x _ { i } ) + f _ { t } ^ { 2 } ( x _ { i } ) ] + \Omega ( f _ { t } ) + constant \\
&amp;= \sum _ { i = 1 } ^ { n } \frac { 1 } { 2 } h _ { i } [ ( 2 \cdot \frac { g _ { i } } { h _ { i } } \cdot f _ { t } ( x _ { i } ) + f _ { t } ^ { 2 } ( x _ { i } ) + ( \frac { g _ { i } } { h _ { i } } ) ^ { 2 } ) - ( \frac { g _ { i } } { h _ { i } } ) ^ { 2 } ] + \Omega ( f _ { t } ) + constant \\
&amp;= \sum _ { i = 1 } ^ { n } \frac { 1 } { 2 } h _ { i } [ ( f _ { t } ( x _ { i } ) + \frac { g _ { i } } { h _ { i } } ) ^ { 2 } ] + \Omega ( f _ { t } ) + constant \\
&amp;= \sum _ { i = 1 } ^ { n } \frac { 1 } { 2 } h _ { i } ( f _ { t } ( x _ { i } ) - ( - g _ { i } / h _ { i } ) ) ^ { 2 } + \Omega ( f _ { t } )
\end{split}
\]</span> 这里和原文给出的公式差个负号，欢迎想明白的小伙伴留言。</p>
<p>从上式中我们可以看到样本<span class="math inline">\(x _ {i}\)</span>的预测值<span class="math inline">\(f _ { t } ( x _ { i })\)</span>对 <span class="math inline">\(y = - g _ { i } / h _ { i }\)</span>的加权平方损失。权重即为<span class="math inline">\(h _ {i}\)</span>，表示样本对loss的影响。</p>
<p>节点上特征值小于 <span class="math inline">\(z\)</span> 的样本集相对于节点上总样本集对loss影响的比重为： <span class="math display">\[
r _ { k } ( z ) = \frac { 1 } { \sum _ { ( x , h ) \in D _ { k } } h } \sum _ { ( x , h ) \in D _ { k } , x &lt; z } h
\]</span> 我们的目标是要找到这样一组切分点<span class="math inline">\(\{ s _ { k 1 } , s _ { k 2 } , \cdots s _ { k l } \}\)</span>, 使用任意两个切分点之间的样本集对loss影响的比重小于一个数值<span class="math inline">\(\epsilon\)</span>。比如 <span class="math inline">\(\epsilon = 10\%\)</span>，意思就是将特征值分为10桶，每桶内对应的样本集对loss的影响小于10%。形式化表示如下： <span class="math display">\[
| r _ { k } ( s _ { k , j } ) - r _ { k } ( s _ { k , j + 1 } ) | &lt; \epsilon , \quad s _ { k 1 } = \min _ { i } x _ { i k } , s _ { k l } = \max _ { i } x _ { i k }
\]</span></p>
<h3 id="稀疏问题的处理方法">稀疏问题的处理方法</h3>
<hr />
<p>在实践中经常遇到的数据稀疏，某些样本的某些特征值是缺失的（NULL）或大量特征的特征值为0。对这种情况论文提出统一将这种数据划分到一个默认的分支，或左树或右树，然后分别计算损失，选择较优的那一个。</p>
<h3 id="xgboost系统优化">XGBoost系统优化</h3>
<hr />
<ul>
<li><p>针对并行计算的列块</p>
<ul>
<li><p>分<code>block</code>，便于并行化</p></li>
<li><p>分列存放，便于按列切片，实施 "列采样"</p></li>
<li><p>按照特征值事先排好序，便于执行切分</p></li>
<li><ul>
<li>在乱序列表上查一个数，只能是线性扫描，复杂度 <span class="math inline">\(O ( N )\)</span></li>
<li>在有序列表上可以做二分查找， <span class="math inline">\(O ( \log N )\)</span></li>
</ul></li>
</ul></li>
<li><p>针对 cpu缓存优化</p>
<ul>
<li>贪心算法：缓存预取缓解缓存未命中问题</li>
<li>近似算法：减小块的大小来解决缓存未命中问题</li>
</ul></li>
<li><p>针对IO优化</p>
<ul>
<li>Block Compression：block按列压缩，在加载到内存的过程中完成解压缩。</li>
<li>Block Sharding：将数据分片到多个磁盘上。 将预取线程分配给每个磁盘，并将数据取入内存缓冲区中。 然后，训练线程可以从每个缓冲区读取数据。 当有多个磁盘可用时，这有助于提高磁盘读取的吞吐量。</li>
</ul></li>
</ul>
<h3 id="总结">总结</h3>
<hr />
<p>个人觉得XGBoost在面对目标函数很难被计算时的思路很有些意思。在看论文的过程中几次被这种奇思妙想击中，总结一下以便时不时回顾感叹一下：</p>
<ul>
<li>惩罚项是棵树时的处理</li>
<li>泰勒展示恰到好处的应用</li>
<li><span class="math inline">\(I_{j}\)</span> 的定义把<span class="math inline">\(Obj\)</span> 神奇地转化成了二次表达式</li>
</ul>
<p>此外，在给特征值分桶时，用对loss的影响做为标准也是个不错的想法，并且找到用<span class="math inline">\(h\)</span>做为权重也是漂亮的一波操作。</p>
<p>以上是个人觉得这篇论文最为出彩的地方，欢迎小伙伴留言分享你的心得。</p>
<h3 id="参考文献">参考文献</h3>
<hr />
<ol type="1">
<li><a href="https://dl.acm.org/doi/pdf/10.1145/2939672.2939785" target="_blank" rel="noopener">XGBoost: A Scalable Tree Boosting System</a></li>
<li><a href="https://netman.aiops.org/~peidan/ANM2018/3.MachineLearningBasics/LectureCoverage/18.xgboost.pdf" target="_blank" rel="noopener">PPT</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/89546007" target="_blank" rel="noopener">详解《XGBoost: A Scalable Tree Boosting System》</a></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-leanring/" rel="tag"># Machine leanring</a>
              <a href="/tags/XGBoost/" rel="tag"># XGBoost</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/05/23/hello-world/" rel="prev" title="Hello World">
      <i class="fa fa-chevron-left"></i> Hello World
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/06/04/WMD-From-Word-Embeddings-To-Document-Distances/" rel="next" title="WMD论文总结及代码实现:  From Word Embeddings To Document Distances">
      WMD论文总结及代码实现:  From Word Embeddings To Document Distances <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>

  
  
  




          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#xgboost目标函数推导"><span class="nav-number">1.</span> <span class="nav-text">XGboost目标函数推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#预测值"><span class="nav-number">1.1.</span> <span class="nav-text">预测值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#目标函数"><span class="nav-number">1.2.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#训练方法"><span class="nav-number">1.3.</span> <span class="nav-text">训练方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#目标函数近似形式"><span class="nav-number">1.4.</span> <span class="nav-text">目标函数近似形式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#目标函数求最优"><span class="nav-number">1.5.</span> <span class="nav-text">目标函数求最优</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#防止过拟合策略"><span class="nav-number">2.</span> <span class="nav-text">防止过拟合策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#切分搜索算法"><span class="nav-number">3.</span> <span class="nav-text">切分搜索算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#贪心算法"><span class="nav-number">3.1.</span> <span class="nav-text">贪心算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#近似算法"><span class="nav-number">3.2.</span> <span class="nav-text">近似算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#加权分位点"><span class="nav-number">3.3.</span> <span class="nav-text">加权分位点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#稀疏问题的处理方法"><span class="nav-number">4.</span> <span class="nav-text">稀疏问题的处理方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#xgboost系统优化"><span class="nav-number">5.</span> <span class="nav-text">XGBoost系统优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">6.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考文献"><span class="nav-number">7.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Adline125"
      src="/images/icon.png">
  <p class="site-author-name" itemprop="name">Adline125</p>
  <div class="site-description" itemprop="description">北京程序媛一枚，主攻NLP.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Adline125" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Adline125" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:gglfxsld@gmail.com" title="E-Mail → mailto:gglfxsld@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Adline125</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>
<br/>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次 | 本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  var disqus_config = function() {
    this.page.url = "http://yoursite.com/2020/06/01/XGBoost-A-Scalable-Tree-Boosting-System/";
    this.page.identifier = "2020/06/01/XGBoost-A-Scalable-Tree-Boosting-System/";
    this.page.title = "XGBoost: A Scalable Tree Boosting System";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://Adline125.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
